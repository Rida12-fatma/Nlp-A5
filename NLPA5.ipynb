{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c594ade-9c93-4229-ac1f-4102998d9ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d1ebf270094ef698ee0834787719e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eff8034a4774c8fabc29e129cfc4cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80361c380db4f7ab03a6ec5dc36c5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'post_id': 'himc90', 'domain': 'askacademia_train', 'upvote_ratio': 0.99, 'history': 'In an interview right before receiving the 2013 Nobel prize in physics, Peter Higgs stated that he wouldn\\'t be able to get an academic job today, because he wouldn\\'t be regarded as productive enough. > By the time he retired in 1996, he was uncomfortable with the new academic culture. \"After I retired it was quite a long time before I went back to my department. I thought I was well out of it. It wasn\\'t my way of doing things any more. Today I wouldn\\'t get an academic job. It\\'s as simple as that. I don\\'t think I would be regarded as productive enough.\"  Another interesting quote from the article is the following:  > He doubts a similar breakthrough could be achieved in today\\'s academic culture, because of the expectations on academics to collaborate and keep churning out papers. He said: \"It\\'s difficult to imagine how I would ever have enough peace and quiet in the present sort of climate to do what I did in 1964.\"  Source (the whole article is pretty interesting): http://theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system', 'c_root_id_A': 'fwhnqat', 'c_root_id_B': 'fwhp8d4', 'created_at_utc_A': 1593535113, 'created_at_utc_B': 1593535824, 'score_A': 52, 'score_B': 54, 'human_ref_A': 'Currently wrapping up my PhD. There is a stark difference in work balance life between students in my lab who are focused on industry and those focused on academia. The ones in academia feel an immense stress to get high level publications (some staying 8+ years to try to push something into nature/science). The competition has become cut throat. This is a trend not just in America but in Europe, Asia and middle east. International graduate students tell me in China go back 20 years, having any ACS publication from american university is enough to get professorship. Now you better come stacked with publications and at least one nature/science. American universities are even more competitive. How many publications, how many conferences, how many patents...', 'human_ref_B': 'It’s ironic to me that research has shown that productivity isn’t all it’s cracked up to be yet here we are.', 'labels': 0, 'seconds_difference': 711.0, 'score_ratio': 1.0384615385}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SHP dataset\n",
    "dataset = load_dataset(\"stanfordnlp/SHP\", split=\"train\")\n",
    "\n",
    "# Show an example\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6fe7d1-f2ac-4d51-a87b-e8fb0caf638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post_id', 'domain', 'upvote_ratio', 'history', 'c_root_id_A', 'c_root_id_B', 'created_at_utc_A', 'created_at_utc_B', 'score_A', 'score_B', 'human_ref_A', 'human_ref_B', 'labels', 'seconds_difference', 'score_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Print dataset features (column names)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29917427-ed28-4d67-b68b-99db622c0809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  In an interview right before receiving the 201...   \n",
      "1  If any professor is reading this: please do no...   \n",
      "2  If any professor is reading this: please do no...   \n",
      "3  If any professor is reading this: please do no...   \n",
      "4  If any professor is reading this: please do no...   \n",
      "\n",
      "                                          response_1  \\\n",
      "0  Currently wrapping up my PhD. There is a stark...   \n",
      "1  And when your teacher doesn't listen or pay at...   \n",
      "2                Profs can be oblivious? What’s new!   \n",
      "3  This sounds like a problem with a specific pro...   \n",
      "4  This would be totally unacceptable in my class...   \n",
      "\n",
      "                                          response_2 preferred_response  \n",
      "0  It’s ironic to me that research has shown that...        human_ref_A  \n",
      "1  I'm pretty strict on time, to the point where ...        human_ref_A  \n",
      "2  This sounds like a problem with a specific pro...        human_ref_A  \n",
      "3  And when your teacher doesn't listen or pay at...        human_ref_B  \n",
      "4  This sounds like a problem with a specific pro...        human_ref_A  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # Map labels to indicate the preferred response\n",
    "    df[\"preferred_response\"] = df[\"labels\"].apply(lambda x: \"human_ref_A\" if x == 0 else \"human_ref_B\")\n",
    "\n",
    "    # Rename columns to match DPO format\n",
    "    df = df.rename(columns={\"history\": \"prompt\", \"human_ref_A\": \"response_1\", \"human_ref_B\": \"response_2\"})\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    df = df[[\"prompt\", \"response_1\", \"response_2\", \"preferred_response\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "df = preprocess_dataset(dataset)\n",
    "\n",
    "# Show sample\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404073df-8402-41a5-857f-d4847be25e1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd3d9db-8be7-4e30-8064-34f4db65c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4d74bd-46fb-470d-8476-871a115469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b94a0c-47ba-4a0a-b67a-fb7dd52272ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set padding token explicitly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ae138f-fdef-4313-b8d1-96282bf4a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=100):\n",
    "    print(\"Encoding input...\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    print(\"Encoded input:\", inputs)\n",
    "\n",
    "    print(\"Generating text...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    print(\"Generated token IDs:\", outputs)\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nFinal Generated Text:\\n\", generated_text)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743b1131-3237-4adb-8826-ea0f7a3f54c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding input...\n",
      "Encoded input: {'input_ids': tensor([[7454, 2402,  257,  640]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "Generating text...\n",
      "Generated token IDs: tensor([[ 7454,  2402,   257,   640,   340,   373,   257,  6283,    11,  6283,\n",
      "          5440,    11,   290,   663,  5085,  1444,   340,   511,  1363,    13,\n",
      "           632,   373,   257,  1295,   326,  6204,   290,   373,   783,  1444,\n",
      "           366,    35,  7484,  1600,   257,  3381, 33115,   287,  9768,   416,\n",
      "           262,  2739,  1605, 47603,  8124, 49381,   508,   318,  1900,   355,\n",
      "           530,   286,   262, 20976, 27642,   286,  8615, 29126,    13,   198,\n",
      "           198,    35,  7484,   318,   257,  1295,  1900,   287,   617,  3354,\n",
      "           286,   262,   995,   355,   852,   845,  8756,    11,   351,   645,\n",
      "          2126,   703,   881,  2392,   340,  1244,   307, 49055,    13,   198,\n",
      "           198,  2061,   389,   262, 15587,   286,   534,  1438,    30,   198]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Final Generated Text:\n",
      " Once upon a time it was a strange, strange planet, and its residents called it their home. It was a place that stood and was now called \"Diana\", a term coined in 1992 by the late American astronomer Carl Sagan who is known as one of the foremost proponents of cosmology.\n",
      "\n",
      "Diana is a place known in some parts of the world as being very alien, with no idea how much longer it might be habitable.\n",
      "\n",
      "What are the origins of your name?\n",
      "\n",
      "Once upon a time it was a strange, strange planet, and its residents called it their home. It was a place that stood and was now called \"Diana\", a term coined in 1992 by the late American astronomer Carl Sagan who is known as one of the foremost proponents of cosmology.\n",
      "\n",
      "Diana is a place known in some parts of the world as being very alien, with no idea how much longer it might be habitable.\n",
      "\n",
      "What are the origins of your name?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91733e-af74-4816-b417-ce75a39751b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11396065-494b-43d3-97b1-3f7548baa786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_trained_model/tokenizer_config.json',\n",
       " 'my_trained_model/special_tokens_map.json',\n",
       " 'my_trained_model/vocab.json',\n",
       " 'my_trained_model/merges.txt',\n",
       " 'my_trained_model/added_tokens.json',\n",
       " 'my_trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you used a model like GPT-2 and fine-tuned it\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your fine-tuned model (replace \"gpt2\" with your model name if different)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Replace with your trained model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace with your tokenizer if needed\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"my_trained_model\")\n",
    "tokenizer.save_pretrained(\"my_trained_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038105c3-d29f-459e-9d5e-2b749d2ee82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86499ab-2ba2-45ae-9666-ade496fd801a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
